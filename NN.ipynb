{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from grad_check.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import import_ipynb\n",
    "import grad_check as gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [28 * 28, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_back(dA, z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return dA * s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_back(dA, z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[z<=0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layer_dims):\n",
    "    L = len(layer_dims)\n",
    "    params = {}\n",
    "    for l in range(1, L):\n",
    "        params[\"w\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * math.sqrt(2/layer_dims[l-1])\n",
    "        params[\"b\" + str(l)] = np.zeros([layer_dims[l], 1])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(trainx, trainy, size):\n",
    "    m = trainx.shape[1]\n",
    "    n_mb = math.floor(m / size)\n",
    "    minibatch_x = []\n",
    "    minibatch_y = []\n",
    "    \n",
    "    for i in range(n_mb):\n",
    "        x = trainx[:, i * size : (i + 1) * size]\n",
    "        y = trainy[:, i * size : (i + 1) * size]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    if m % size != 0:\n",
    "        x = trainx[:, n_mb :]\n",
    "        y = trainy[:, n_mb :]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    return minibatch_x, minibatch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_prop(params, x, Y):\n",
    "    A_prev = np.array(x, copy = True)\n",
    "    m = x.shape[1]\n",
    "    L = len(params)//2\n",
    "    caches = []\n",
    "    for l in range(L - 1):\n",
    "        \n",
    "        w = params['w' + str(l+1)]\n",
    "        b = params['b' + str(l+1)]\n",
    "        \n",
    "        \n",
    "        assert(w.shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(b.shape == (layer_dims[l + 1], 1))\n",
    "        assert(A_prev.shape == (layer_dims[l], m))\n",
    "            \n",
    "            \n",
    "        z = np.dot(w, A_prev) + b\n",
    "        cache = (A_prev, w, b, z)\n",
    "        A_prev = relu(z)\n",
    "        \n",
    "        \n",
    "        assert(z.shape == (layer_dims[l + 1], m))\n",
    "        \n",
    "        caches.append(cache)\n",
    "    \n",
    "        \n",
    "    w = params[\"w\" + str(L)]\n",
    "    b = params[\"b\" + str(L)]\n",
    "    \n",
    "    assert(w.shape == (layer_dims[L], layer_dims[L - 1]))\n",
    "    assert(b.shape == (layer_dims[L], 1))\n",
    "    assert(A_prev.shape == (layer_dims[L-1], m))\n",
    "    z = np.dot(w, A_prev) + b\n",
    "    \n",
    "    assert(z.shape == (layer_dims[L], m))\n",
    "    \n",
    "    cache = (A_prev, w, b, z)\n",
    "    AL = sigmoid(z)\n",
    "    \n",
    "    cost = -(np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)))/m\n",
    "    \n",
    "    caches.append(cache)\n",
    "    return cost, AL, caches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(AL, Y, caches):\n",
    "    m = Y.shape[1]\n",
    "    dA = np.divide(-Y, AL) + np.divide(1 - Y, 1 - AL)\n",
    "    assert(dA.shape == (layer_dims[-1], m))\n",
    "    grads = []\n",
    "    \n",
    "    L = len(caches)\n",
    "    A_prev, w, b, z = caches[L -1]\n",
    "    \n",
    "    dZ = sigmoid_back(dA, z)\n",
    "    dw = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(dZ.shape == z.shape)\n",
    "    dA_prev = np.dot(w.T, dZ)\n",
    "    dA = dA_prev\n",
    "    \n",
    "    grad = {\"dZ\" + str(L): dZ,\n",
    "           \"dw\" + str(L): dw,\n",
    "           \"db\" + str(L): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#     grad = (dZ, dw, db, dA_prev)\n",
    "    grads.append(grad)\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        A_prev, w, b, z = caches[l]\n",
    "        \n",
    "        dZ = relu_back(dA, z)\n",
    "        dw = np.dot(dZ, A_prev.T)/m\n",
    "        db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "        assert(db.shape == b.shape)\n",
    "        assert(dw.shape == w.shape)\n",
    "        assert(dZ.shape == z.shape)\n",
    "        dA_prev = np.dot(w.T, dZ)\n",
    "        dA = dA_prev\n",
    "        \n",
    "        grad = {\"dZ\" + str(l + 1): dZ,\n",
    "           \"dw\" + str(l + 1): dw,\n",
    "           \"db\" + str(l + 1): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#         grad = (dZ, dw, db, dA_prev)\n",
    "        grads.append(grad)\n",
    "        \n",
    "        \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(params, grads, X, Y, epsilon = 1e-7):\n",
    "    theta_values, keys_theta = gc.dictionary_to_vector(params)\n",
    "    grad = gc.gradient_to_vector(grads)\n",
    "    L = theta_values.shape[0]\n",
    "    grad_approx = np.zeros((L, 1))\n",
    "    \n",
    "    for i in range(L):\n",
    "        theta_plus = np.array(theta_values, copy = True)\n",
    "        theta_plus[i][0] = theta_plus[i][0] + epsilon\n",
    "        J_plus,_,_ = for_prop(gc.vector_to_dictionary(theta_plus, keys_theta, params), X, Y)\n",
    "        \n",
    "        theta_minus = np.array(theta_values, copy = True)\n",
    "        theta_minus[i][0] = theta_minus[i][0] - epsilon\n",
    "        J_minus,_,_ = for_prop(gc.vector_to_dictionary(theta_minus, keys_theta, params), X, Y)\n",
    "        \n",
    "        grad_approx[i] = (J_plus - J_minus)/(2 * epsilon)\n",
    "        \n",
    "    numerator = np.linalg.norm(grad - grad_approx)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        print(\"There is a mistake in back propagation, difference: \", str(difference))\n",
    "    else:\n",
    "        print(\"You nailed it, back propagation is all perfect Ready to GO..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(params, grads, learning_rate):\n",
    "    L = len(params)//2\n",
    "    # if you use gradient checking then grads.reverse() is performed in the gradient_to_vector method in grad_check so don't need to call the reverse method in here\n",
    "    \n",
    "    #if you are not using gradient checking then you have to reverse grads here...\n",
    "    grads.reverse()\n",
    "    \n",
    "    for l in range(L):\n",
    "        grad = grads[l]\n",
    "        dw = grad['dw' + str(l + 1)]\n",
    "        db = grad['db' + str(l + 1)]\n",
    "#         dZ, dw, db, dA_prev = grad\n",
    "        params['w' + str(l + 1)] = params['w' + str(l + 1)] - (learning_rate * dw)\n",
    "        params['b' + str(l + 1)] = params['b' + str(l + 1)] - (learning_rate * db)\n",
    "\n",
    "        assert(params['w' + str(l + 1)].shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(params['b' + str(l + 1)].shape == (layer_dims[l + 1], 1))\n",
    "        \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, x, y):\n",
    "    cost, AL, caches = for_prop(params, x, y)\n",
    "    \n",
    "    y_prediction = np.where(AL<=0.5, 0, 1)\n",
    "    acc = 100 - (np.mean(np.abs(y_prediction - y)) * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(trianx, trainy, testx, testy, layer_dims, num_iteration, learning_rate, minibatch_size, print_cost):\n",
    "    params = initialize(layer_dims)\n",
    "    \n",
    "    costs = []\n",
    "    for i in range(1, num_iteration+1):\n",
    "        \n",
    "        minibatch_x, minibatch_y = create_minibatch(trainx, trainy, minibatch_size)\n",
    "        cost_minibatch = 0\n",
    "        \n",
    "        for l in range(len(minibatch_x)):\n",
    "            cost, AL, caches = for_prop(params, minibatch_x[l], minibatch_y[l])\n",
    "            cost_minibatch += cost\n",
    "            grads = back_prop(AL, minibatch_y[l], caches)\n",
    "    #         gradient_check(params, grads, trainx, trainy, 1e-7)\n",
    "            params = optimize(params, grads, learning_rate)\n",
    "        \n",
    "        cost_minibatch = cost_minibatch / len(minibatch_x)\n",
    "\n",
    "            \n",
    "        if i%100==0 and print_cost:\n",
    "            print(\"cost at iteration \",i ,\": \",cost)\n",
    "            costs.append(cost_minibatch)\n",
    "            \n",
    "    train_acc = predict(params, trainx, trainy)\n",
    "    test_acc = predict(params, testx, testy)\n",
    "    \n",
    "    print(\"Training accuracy: \", train_acc)\n",
    "    print(\"Test accuracy: \", test_acc)\n",
    "    \n",
    "    data = {\"params\": params,\n",
    "           \"costs\": costs,\n",
    "            \"grads\": grads,\n",
    "           \"train_acc\": train_acc,\n",
    "           \"test_acc\": test_acc}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"handwrittendata.pickle\", \"rb\") as pickle_in:\n",
    "    data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['trainx', 'trainy', 'testx', 'testy'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data[\"trainx\"]\n",
    "trainy = data[\"trainy\"]\n",
    "testx = data[\"testx\"]\n",
    "testy = data[\"testy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_train = np.random.permutation(trainx.shape[1])\n",
    "permutation_test = np.random.permutation(testx.shape[1])\n",
    "\n",
    "trainx = trainx[:, permutation_train]\n",
    "trainy = trainy[:, permutation_train].reshape(1, trainx.shape[1])\n",
    "testx = testx[:, permutation_test]\n",
    "testy = testy[:, permutation_test].reshape(1, testx.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e565fc58c8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = trainx[:, 108]\n",
    "img = img.reshape(28, 28)\n",
    "plt.imshow(img, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data['trainx']/255\n",
    "trainy = data['trainy']\n",
    "testx = data['testx']/255\n",
    "testy = data['testy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration  100 :  0.04479433237617309\n",
      "cost at iteration  200 :  0.03279755324368797\n",
      "cost at iteration  300 :  0.02703777271128906\n",
      "cost at iteration  400 :  0.02336678685237098\n",
      "cost at iteration  500 :  0.020736218925935652\n",
      "cost at iteration  600 :  0.018726594950645936\n",
      "cost at iteration  700 :  0.0171270545312014\n",
      "cost at iteration  800 :  0.015816427119028975\n",
      "cost at iteration  900 :  0.014718858987888566\n",
      "cost at iteration  1000 :  0.013783912948076047\n",
      "cost at iteration  1100 :  0.012976472347074344\n",
      "cost at iteration  1200 :  0.01227117434266175\n",
      "cost at iteration  1300 :  0.011649147728753088\n",
      "cost at iteration  1400 :  0.011096004923527497\n",
      "cost at iteration  1500 :  0.010600552876671868\n",
      "cost at iteration  1600 :  0.01015393474771043\n",
      "cost at iteration  1700 :  0.009749040119470705\n",
      "cost at iteration  1800 :  0.009380088656413084\n",
      "cost at iteration  1900 :  0.009042329357626675\n",
      "cost at iteration  2000 :  0.008731818982772099\n",
      "Training accuracy:  99.95002498750625\n",
      "Test accuracy:  98.75\n"
     ]
    }
   ],
   "source": [
    "#use small neural network for a simple problems\n",
    "#if you use larger neural network then \n",
    "d = model(trainx, trainy, testx, testy, layer_dims, 2000, 0.05, 128, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN_result.pickle\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['train_acc'])\n",
    "print(d['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN ABClassifier result\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

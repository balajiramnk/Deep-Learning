{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import import_ipynb\n",
    "import grad_check as gc\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [28 * 28, 4, 3, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    t = np.exp(z)\n",
    "    z = np.sum(t, axis = 0)\n",
    "    return np.divide(t, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_back(dA, z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[z<=0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layer_dims):\n",
    "    L = len(layer_dims)\n",
    "    params = {}\n",
    "    for l in range(1, L):\n",
    "        params[\"w\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * math.sqrt(2/layer_dims[l-1])\n",
    "        params[\"b\" + str(l)] = np.zeros([layer_dims[l], 1])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(params):\n",
    "    v = {}\n",
    "    s = {}\n",
    "    L = len(params)//2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dw\" + str(l)] = np.zeros(params[\"w\" + str(l)].shape)\n",
    "        v[\"db\" + str(l)] = np.zeros(params[\"b\" + str(l)].shape)\n",
    "        \n",
    "        s[\"dw\" + str(l)] = np.zeros(params[\"w\" + str(l)].shape)\n",
    "        s[\"db\" + str(l)] = np.zeros(params[\"b\" + str(l)].shape)\n",
    "        \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(trainx, trainy, size):\n",
    "    m = trainx.shape[1]\n",
    "    n_mb = math.floor(m / size)\n",
    "    minibatch_x = []\n",
    "    minibatch_y = []\n",
    "    \n",
    "    for i in range(n_mb):\n",
    "        x = trainx[:, i * size : (i + 1) * size]\n",
    "        y = trainy[:, i * size : (i + 1) * size]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    if m % size != 0:\n",
    "        x = trainx[:, n_mb :]\n",
    "        y = trainy[:, n_mb :]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    return minibatch_x, minibatch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_prop(params, x, Y):\n",
    "    A_prev = np.array(x, copy = True)\n",
    "    m = x.shape[1]\n",
    "    L = len(params)//2\n",
    "    caches = []\n",
    "    for l in range(L - 1):\n",
    "        \n",
    "        w = params['w' + str(l+1)]\n",
    "        b = params['b' + str(l+1)]\n",
    "        \n",
    "        \n",
    "        assert(w.shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(b.shape == (layer_dims[l + 1], 1))\n",
    "        assert(A_prev.shape == (layer_dims[l], m))\n",
    "            \n",
    "            \n",
    "        z = np.dot(w, A_prev) + b\n",
    "        cache = (A_prev, w, b, z)\n",
    "        A_prev = relu(z)\n",
    "        \n",
    "        \n",
    "        assert(z.shape == (layer_dims[l + 1], m))\n",
    "        \n",
    "        caches.append(cache)\n",
    "    \n",
    "        \n",
    "    w = params[\"w\" + str(L)]\n",
    "    b = params[\"b\" + str(L)]\n",
    "    \n",
    "    assert(w.shape == (layer_dims[L], layer_dims[L - 1]))\n",
    "    assert(b.shape == (layer_dims[L], 1))\n",
    "    assert(A_prev.shape == (layer_dims[L-1], m))\n",
    "    z = np.dot(w, A_prev) + b\n",
    "    \n",
    "    assert(z.shape == (layer_dims[L], m))\n",
    "    \n",
    "    cache = (A_prev, w, b, z)\n",
    "    AL = softmax(z)\n",
    "    \n",
    "    cost = -(np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)))/m\n",
    "    \n",
    "    caches.append(cache)\n",
    "    return cost, AL, caches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(AL, Y, caches):\n",
    "    m = Y.shape[1]\n",
    "    grads = []\n",
    "    \n",
    "    L = len(caches)\n",
    "    A_prev, w, b, z = caches[L -1]\n",
    "    \n",
    "    dZ = AL - Y\n",
    "    dw = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(dZ.shape == z.shape)\n",
    "    dA_prev = np.dot(w.T, dZ)\n",
    "    dA = dA_prev\n",
    "    \n",
    "    grad = {\"dZ\" + str(L): dZ,\n",
    "           \"dw\" + str(L): dw,\n",
    "           \"db\" + str(L): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#     grad = (dZ, dw, db, dA_prev)\n",
    "    grads.append(grad)\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        A_prev, w, b, z = caches[l]\n",
    "        \n",
    "        dZ = relu_back(dA, z)\n",
    "        dw = np.dot(dZ, A_prev.T)/m\n",
    "        db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "        assert(db.shape == b.shape)\n",
    "        assert(dw.shape == w.shape)\n",
    "        assert(dZ.shape == z.shape)\n",
    "        dA_prev = np.dot(w.T, dZ)\n",
    "        dA = dA_prev\n",
    "        \n",
    "        grad = {\"dZ\" + str(l + 1): dZ,\n",
    "           \"dw\" + str(l + 1): dw,\n",
    "           \"db\" + str(l + 1): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#         grad = (dZ, dw, db, dA_prev)\n",
    "        grads.append(grad)\n",
    "        \n",
    "        \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(params, grads, X, Y, epsilon = 1e-7):\n",
    "    theta_values, keys_theta = gc.dictionary_to_vector(params)\n",
    "    grad = gc.gradient_to_vector(grads)\n",
    "    L = theta_values.shape[0]\n",
    "    grad_approx = np.zeros((L, 1))\n",
    "    \n",
    "    for i in range(L):\n",
    "        theta_plus = np.array(theta_values, copy = True)\n",
    "        theta_plus[i][0] = theta_plus[i][0] + epsilon\n",
    "        J_plus,_,_ = for_prop(gc.vector_to_dictionary(theta_plus, keys_theta, params), X, Y)\n",
    "        \n",
    "        theta_minus = np.array(theta_values, copy = True)\n",
    "        theta_minus[i][0] = theta_minus[i][0] - epsilon\n",
    "        J_minus,_,_ = for_prop(gc.vector_to_dictionary(theta_minus, keys_theta, params), X, Y)\n",
    "        \n",
    "        grad_approx[i] = (J_plus - J_minus)/(2 * epsilon)\n",
    "        \n",
    "    numerator = np.linalg.norm(grad - grad_approx)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        print(\"There is a mistake in back propagation, difference: \", str(difference))\n",
    "    else:\n",
    "        print(\"You nailed it, back propagation is all perfect Ready to GO..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(params, grads, v, s, beta1, beta2, t, epsilon, learning_rate):\n",
    "    L = len(params)//2\n",
    "    # if you use gradient checking then grads.reverse() is performed in the gradient_to_vector method in grad_check so don't need to call the reverse method in here\n",
    "    \n",
    "    #if you are not using gradient checking then you have to reverse grads here...\n",
    "    grads.reverse()\n",
    "    \n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        grad = grads[l]\n",
    "        \n",
    "        v[\"dw\" + str(l + 1)] = (beta1 * v[\"dw\" + str(l + 1)]) + ((1 - beta1) * grad[\"dw\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = (beta1 * v[\"db\" + str(l + 1)]) + ((1 - beta1) * grad[\"db\" + str(l + 1)])\n",
    "        \n",
    "        v_corrected[\"dw\" + str(l + 1)] = v[\"dw\" + str(l + 1)]/(1 - beta1**t)\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)]/(1 - beta1**t)\n",
    "        \n",
    "        s[\"dw\" + str(l + 1)] = (beta2 * s[\"dw\" + str(l + 1)]) + ((1 - beta2) * (grad[\"dw\" + str(l + 1)]**2))\n",
    "        s[\"db\" + str(l + 1)] = (beta2 * s[\"db\" + str(l + 1)]) + ((1 - beta2) * (grad[\"db\" + str(l + 1)]**2))\\\n",
    "        \n",
    "        s_corrected[\"dw\" + str(l + 1)] = s[\"dw\" + str(l + 1)]/(1 - beta2**t)\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)]/(1 - beta2**t)\n",
    "        \n",
    "        params[\"w\" + str(l + 1)] = params[\"w\" + str(l + 1)] - (learning_rate * (v_corrected[\"dw\" + str(l + 1)]/np.sqrt(s_corrected[\"dw\" + str(l + 1)] + epsilon)))\n",
    "        params[\"b\" + str(l + 1)] = params[\"b\" + str(l + 1)] - (learning_rate * (v_corrected[\"db\" + str(l + 1)]/np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)))\n",
    "        \n",
    "        \n",
    "        assert(params['w' + str(l + 1)].shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(params['b' + str(l + 1)].shape == (layer_dims[l + 1], 1))\n",
    "        \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, x, y):\n",
    "    cost, AL, caches = for_prop(params, x, y)\n",
    "    \n",
    "    Y_prediction = np.zeros(AL.shape)\n",
    "    for i in range(AL.shape[1]):\n",
    "        Y_prediction[np.argmax(AL[:, i]), i] = 1\n",
    "    \n",
    "    acc = ((Y_prediction * y).sum()/y.shape[1]) * 100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(trianx, trainy, testx, testy, layer_dims, num_iteration, learning_rate, minibatch_size, print_cost, beta1 = 0.9, beta2 = 0.999, epsilon = 10e-8):\n",
    "    params = initialize(layer_dims)\n",
    "    v, s = initialize_adam(params)\n",
    "    \n",
    "    costs = []\n",
    "    for i in range(1, num_iteration+1):\n",
    "        \n",
    "        minibatch_x, minibatch_y = create_minibatch(trainx, trainy, minibatch_size)\n",
    "        cost_minibatch = 0\n",
    "        t = 0\n",
    "        for l in range(len(minibatch_x)):\n",
    "            cost, AL, caches = for_prop(params, minibatch_x[l], minibatch_y[l])\n",
    "            cost_minibatch += cost\n",
    "            grads = back_prop(AL, minibatch_y[l], caches)\n",
    "    #         gradient_check(params, grads, trainx, trainy, 1e-7)\n",
    "            t = t + 1\n",
    "            params = optimize(params, grads, v, s, beta1, beta2, t, epsilon, learning_rate)\n",
    "        \n",
    "        cost_minibatch = cost_minibatch / len(minibatch_x)\n",
    "\n",
    "            \n",
    "        if i%100==0 and print_cost:\n",
    "            print(\"cost at iteration \",i ,\": \",cost)\n",
    "            costs.append(cost_minibatch)\n",
    "            \n",
    "    train_acc = predict(params, trainx, trainy)\n",
    "    test_acc = predict(params, testx, testy)\n",
    "    \n",
    "    print(\"Training accuracy: \", train_acc)\n",
    "    print(\"Test accuracy: \", test_acc)\n",
    "    \n",
    "    data = {\"params\": params,\n",
    "           \"costs\": costs,\n",
    "            \"grads\": grads,\n",
    "           \"train_acc\": train_acc,\n",
    "           \"test_acc\": test_acc}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fulalpha.pickle\", \"rb\") as pickle_in:\n",
    "    data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['trainx', 'trainy_c', 'testx', 'testy_c'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data[\"trainx\"]\n",
    "trainy = data[\"trainy_c\"]\n",
    "testx = data[\"testx\"]\n",
    "testy = data[\"testy_c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 13000)\n",
      "(26, 13000)\n",
      "(784, 260)\n",
      "(26, 260)\n"
     ]
    }
   ],
   "source": [
    "print(trainx.shape)\n",
    "print(trainy.shape)\n",
    "print(testx.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_train = np.random.permutation(trainx.shape[1])\n",
    "permutation_test = np.random.permutation(testx.shape[1])\n",
    "\n",
    "trainx = trainx[:, permutation_train]\n",
    "trainy = trainy[:, permutation_train]\n",
    "testx = testx[:, permutation_test]\n",
    "testy = testy[:, permutation_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22680210fc8>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 108\n",
    "img = trainx[:, index]\n",
    "img = img.reshape(28, 28)\n",
    "plt.imshow(img, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data['trainx']/255\n",
    "trainy = data['trainy_c']\n",
    "testx = data['testx']/255\n",
    "testy = data['testy_c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration  100 :  4.183013723615434\n",
      "cost at iteration  200 :  4.181871668383966\n",
      "cost at iteration  300 :  4.181748772763858\n",
      "cost at iteration  400 :  4.181719789340154\n",
      "cost at iteration  500 :  4.18171207889132\n",
      "cost at iteration  600 :  4.181709989724013\n",
      "cost at iteration  700 :  4.181709421624354\n",
      "cost at iteration  800 :  4.181709267012714\n",
      "cost at iteration  900 :  4.1817092249251235\n",
      "cost at iteration  1000 :  4.181709213467595\n",
      "Training accuracy:  3.8461538461538463\n",
      "Test accuracy:  3.8461538461538463\n"
     ]
    }
   ],
   "source": [
    "#use small neural network for a simple problems\n",
    "#if you use larger neural network then \n",
    "d = model(trainx, trainy, testx, testy, layer_dims, 1000, 0.05, 1000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN_result_alpha.pickle\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['train_acc'])\n",
    "print(d['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN ABClassifier result\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

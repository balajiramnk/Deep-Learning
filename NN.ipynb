{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from grad_check.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import import_ipynb\n",
    "import grad_check as gc\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [28 * 28, 4, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_back(dA, z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return dA * s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_back(dA, z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[z<=0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layer_dims):\n",
    "    L = len(layer_dims)\n",
    "    params = {}\n",
    "    for l in range(1, L):\n",
    "        params[\"w\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * math.sqrt(2/layer_dims[l-1])\n",
    "        params[\"b\" + str(l)] = np.zeros([layer_dims[l], 1])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(params):\n",
    "    v = {}\n",
    "    s = {}\n",
    "    L = len(params)//2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dw\" + str(l)] = np.zeros(params[\"w\" + str(l)].shape)\n",
    "        v[\"db\" + str(l)] = np.zeros(params[\"b\" + str(l)].shape)\n",
    "        \n",
    "        s[\"dw\" + str(l)] = np.zeros(params[\"w\" + str(l)].shape)\n",
    "        s[\"db\" + str(l)] = np.zeros(params[\"b\" + str(l)].shape)\n",
    "        \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(trainx, trainy, size):\n",
    "    m = trainx.shape[1]\n",
    "    n_mb = math.floor(m / size)\n",
    "    minibatch_x = []\n",
    "    minibatch_y = []\n",
    "    \n",
    "    for i in range(n_mb):\n",
    "        x = trainx[:, i * size : (i + 1) * size]\n",
    "        y = trainy[:, i * size : (i + 1) * size]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    if m % size != 0:\n",
    "        x = trainx[:, n_mb :]\n",
    "        y = trainy[:, n_mb :]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    return minibatch_x, minibatch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_prop(params, x, Y):\n",
    "    A_prev = np.array(x, copy = True)\n",
    "    m = x.shape[1]\n",
    "    L = len(params)//2\n",
    "    caches = []\n",
    "    for l in range(L - 1):\n",
    "        \n",
    "        w = params['w' + str(l+1)]\n",
    "        b = params['b' + str(l+1)]\n",
    "        \n",
    "        \n",
    "        assert(w.shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(b.shape == (layer_dims[l + 1], 1))\n",
    "        assert(A_prev.shape == (layer_dims[l], m))\n",
    "            \n",
    "            \n",
    "        z = np.dot(w, A_prev) + b\n",
    "        cache = (A_prev, w, b, z)\n",
    "        A_prev = relu(z)\n",
    "        \n",
    "        \n",
    "        assert(z.shape == (layer_dims[l + 1], m))\n",
    "        \n",
    "        caches.append(cache)\n",
    "    \n",
    "        \n",
    "    w = params[\"w\" + str(L)]\n",
    "    b = params[\"b\" + str(L)]\n",
    "    \n",
    "    assert(w.shape == (layer_dims[L], layer_dims[L - 1]))\n",
    "    assert(b.shape == (layer_dims[L], 1))\n",
    "    assert(A_prev.shape == (layer_dims[L-1], m))\n",
    "    z = np.dot(w, A_prev) + b\n",
    "    \n",
    "    assert(z.shape == (layer_dims[L], m))\n",
    "    \n",
    "    cache = (A_prev, w, b, z)\n",
    "    AL = sigmoid(z)\n",
    "    \n",
    "    cost = -(np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)))/m\n",
    "    \n",
    "    caches.append(cache)\n",
    "    return cost, AL, caches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(AL, Y, caches):\n",
    "    m = Y.shape[1]\n",
    "    dA = np.divide(-Y, AL) + np.divide(1 - Y, 1 - AL)\n",
    "    assert(dA.shape == (layer_dims[-1], m))\n",
    "    grads = []\n",
    "    \n",
    "    L = len(caches)\n",
    "    A_prev, w, b, z = caches[L -1]\n",
    "    \n",
    "    dZ = sigmoid_back(dA, z)\n",
    "    dw = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(dZ.shape == z.shape)\n",
    "    dA_prev = np.dot(w.T, dZ)\n",
    "    dA = dA_prev\n",
    "    \n",
    "    grad = {\"dZ\" + str(L): dZ,\n",
    "           \"dw\" + str(L): dw,\n",
    "           \"db\" + str(L): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#     grad = (dZ, dw, db, dA_prev)\n",
    "    grads.append(grad)\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        A_prev, w, b, z = caches[l]\n",
    "        \n",
    "        dZ = relu_back(dA, z)\n",
    "        dw = np.dot(dZ, A_prev.T)/m\n",
    "        db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "        assert(db.shape == b.shape)\n",
    "        assert(dw.shape == w.shape)\n",
    "        assert(dZ.shape == z.shape)\n",
    "        dA_prev = np.dot(w.T, dZ)\n",
    "        dA = dA_prev\n",
    "        \n",
    "        grad = {\"dZ\" + str(l + 1): dZ,\n",
    "           \"dw\" + str(l + 1): dw,\n",
    "           \"db\" + str(l + 1): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#         grad = (dZ, dw, db, dA_prev)\n",
    "        grads.append(grad)\n",
    "        \n",
    "        \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(params, grads, X, Y, epsilon = 1e-7):\n",
    "    theta_values, keys_theta = gc.dictionary_to_vector(params)\n",
    "    grad = gc.gradient_to_vector(grads)\n",
    "    L = theta_values.shape[0]\n",
    "    grad_approx = np.zeros((L, 1))\n",
    "    \n",
    "    for i in range(L):\n",
    "        theta_plus = np.array(theta_values, copy = True)\n",
    "        theta_plus[i][0] = theta_plus[i][0] + epsilon\n",
    "        J_plus,_,_ = for_prop(gc.vector_to_dictionary(theta_plus, keys_theta, params), X, Y)\n",
    "        \n",
    "        theta_minus = np.array(theta_values, copy = True)\n",
    "        theta_minus[i][0] = theta_minus[i][0] - epsilon\n",
    "        J_minus,_,_ = for_prop(gc.vector_to_dictionary(theta_minus, keys_theta, params), X, Y)\n",
    "        \n",
    "        grad_approx[i] = (J_plus - J_minus)/(2 * epsilon)\n",
    "        \n",
    "    numerator = np.linalg.norm(grad - grad_approx)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        print(\"There is a mistake in back propagation, difference: \", str(difference))\n",
    "    else:\n",
    "        print(\"You nailed it, back propagation is all perfect Ready to GO..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(params, grads, v, s, beta1, beta2, t, epsilon, learning_rate):\n",
    "    L = len(params)//2\n",
    "    # if you use gradient checking then grads.reverse() is performed in the gradient_to_vector method in grad_check so don't need to call the reverse method in here\n",
    "    \n",
    "    #if you are not using gradient checking then you have to reverse grads here...\n",
    "    grads.reverse()\n",
    "    \n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        grad = grads[l]\n",
    "        \n",
    "        v[\"dw\" + str(l + 1)] = (beta1 * v[\"dw\" + str(l + 1)]) + ((1 - beta1) * grad[\"dw\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = (beta1 * v[\"db\" + str(l + 1)]) + ((1 - beta1) * grad[\"db\" + str(l + 1)])\n",
    "        \n",
    "        v_corrected[\"dw\" + str(l + 1)] = v[\"dw\" + str(l + 1)]/(1 - beta1**t)\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)]/(1 - beta1**t)\n",
    "        \n",
    "        s[\"dw\" + str(l + 1)] = (beta2 * s[\"dw\" + str(l + 1)]) + ((1 - beta2) * (grad[\"dw\" + str(l + 1)]**2))\n",
    "        s[\"db\" + str(l + 1)] = (beta2 * s[\"db\" + str(l + 1)]) + ((1 - beta2) * (grad[\"db\" + str(l + 1)]**2))\\\n",
    "        \n",
    "        s_corrected[\"dw\" + str(l + 1)] = s[\"dw\" + str(l + 1)]/(1 - beta2**t)\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)]/(1 - beta2**t)\n",
    "        \n",
    "        params[\"w\" + str(l + 1)] = params[\"w\" + str(l + 1)] - (learning_rate * (v_corrected[\"dw\" + str(l + 1)]/np.sqrt(s_corrected[\"dw\" + str(l + 1)] + epsilon)))\n",
    "        params[\"b\" + str(l + 1)] = params[\"b\" + str(l + 1)] - (learning_rate * (v_corrected[\"db\" + str(l + 1)]/np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)))\n",
    "        \n",
    "        \n",
    "        assert(params['w' + str(l + 1)].shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(params['b' + str(l + 1)].shape == (layer_dims[l + 1], 1))\n",
    "        \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, x, y):\n",
    "    cost, AL, caches = for_prop(params, x, y)\n",
    "    \n",
    "    y_prediction = np.where(AL<=0.5, 0, 1)\n",
    "    acc = 100 - (np.mean(np.abs(y_prediction - y)) * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(trianx, trainy, testx, testy, layer_dims, num_iteration, learning_rate, minibatch_size, print_cost, beta1 = 0.9, beta2 = 0.999, epsilon = 10e-8):\n",
    "    params = initialize(layer_dims)\n",
    "    v, s = initialize_adam(params)\n",
    "    \n",
    "    costs = []\n",
    "    for i in range(1, num_iteration+1):\n",
    "        \n",
    "        minibatch_x, minibatch_y = create_minibatch(trainx, trainy, minibatch_size)\n",
    "        cost_minibatch = 0\n",
    "        t = 0\n",
    "        for l in range(len(minibatch_x)):\n",
    "            cost, AL, caches = for_prop(params, minibatch_x[l], minibatch_y[l])\n",
    "            cost_minibatch += cost\n",
    "            grads = back_prop(AL, minibatch_y[l], caches)\n",
    "    #         gradient_check(params, grads, trainx, trainy, 1e-7)\n",
    "            t = t + 1\n",
    "            params = optimize(params, grads, v, s, beta1, beta2, t, epsilon, learning_rate)\n",
    "        \n",
    "        cost_minibatch = cost_minibatch / len(minibatch_x)\n",
    "\n",
    "            \n",
    "        if i%100==0 and print_cost:\n",
    "            print(\"cost at iteration \",i ,\": \",cost)\n",
    "            costs.append(cost_minibatch)\n",
    "            \n",
    "    train_acc = predict(params, trainx, trainy)\n",
    "    test_acc = predict(params, testx, testy)\n",
    "    \n",
    "    print(\"Training accuracy: \", train_acc)\n",
    "    print(\"Test accuracy: \", test_acc)\n",
    "    \n",
    "    data = {\"params\": params,\n",
    "           \"costs\": costs,\n",
    "            \"grads\": grads,\n",
    "           \"train_acc\": train_acc,\n",
    "           \"test_acc\": test_acc}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"handwrittendata.pickle\", \"rb\") as pickle_in:\n",
    "    data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['trainx', 'trainy', 'testx', 'testy'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data[\"trainx\"]\n",
    "trainy = data[\"trainy\"]\n",
    "testx = data[\"testx\"]\n",
    "testy = data[\"testy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 2001)\n",
      "(1, 2001)\n",
      "(784, 80)\n",
      "(1, 80)\n"
     ]
    }
   ],
   "source": [
    "print(trainx.shape)\n",
    "print(trainy.shape)\n",
    "print(testx.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_train = np.random.permutation(trainx.shape[1])\n",
    "permutation_test = np.random.permutation(testx.shape[1])\n",
    "\n",
    "trainx = trainx[:, permutation_train]\n",
    "trainy = trainy[:, permutation_train]\n",
    "testx = testx[:, permutation_test]\n",
    "testy = testy[:, permutation_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x291519db5c8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 108\n",
    "img = trainx[:, index]\n",
    "img = img.reshape(28, 28)\n",
    "plt.imshow(img, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data['trainx']/255\n",
    "trainy = data['trainy']\n",
    "testx = data['testx']/255\n",
    "testy = data['testy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration  100 :  0.6938094919762047\n",
      "cost at iteration  200 :  0.6938106554089215\n",
      "cost at iteration  300 :  0.6938108513455515\n",
      "cost at iteration  400 :  0.6938108896861525\n",
      "cost at iteration  500 :  0.6938108973490842\n",
      "cost at iteration  600 :  0.693810898887036\n",
      "cost at iteration  700 :  0.6938108991959607\n",
      "cost at iteration  800 :  0.6938108992580243\n",
      "cost at iteration  900 :  0.6938108992704937\n",
      "cost at iteration  1000 :  0.6938108992729985\n",
      "Training accuracy:  50.02498750624687\n",
      "Test accuracy:  50.0\n"
     ]
    }
   ],
   "source": [
    "#use small neural network for a simple problems\n",
    "#if you use larger neural network then \n",
    "d = model(trainx, trainy, testx, testy, layer_dims, 1000, 0.05, 128, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN_result.pickle\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['train_acc'])\n",
    "print(d['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN ABClassifier result\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

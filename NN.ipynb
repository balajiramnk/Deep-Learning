{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from grad_check.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import import_ipynb\n",
    "import grad_check as gc\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [28 * 28, 4, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_back(dA, z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return dA * s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_back(dA, z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[z<=0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layer_dims):\n",
    "    L = len(layer_dims)\n",
    "    params = {}\n",
    "    for l in range(1, L):\n",
    "        params[\"w\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * math.sqrt(2/layer_dims[l-1])\n",
    "        params[\"b\" + str(l)] = np.zeros([layer_dims[l], 1])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(params):\n",
    "    v = {}\n",
    "    s = {}\n",
    "    L = len(params)//2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dw\" + str(l)] = np.zeros(params[\"w\" + str(l)].shape)\n",
    "        v[\"db\" + str(l)] = np.zeros(params[\"b\" + str(l)].shape)\n",
    "        \n",
    "        s[\"dw\" + str(l)] = np.zeros(params[\"w\" + str(l)].shape)\n",
    "        s[\"db\" + str(l)] = np.zeros(params[\"b\" + str(l)].shape)\n",
    "        \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(trainx, trainy, size):\n",
    "    m = trainx.shape[1]\n",
    "    n_mb = math.floor(m / size)\n",
    "    minibatch_x = []\n",
    "    minibatch_y = []\n",
    "    \n",
    "    for i in range(n_mb):\n",
    "        x = trainx[:, i * size : (i + 1) * size]\n",
    "        y = trainy[:, i * size : (i + 1) * size]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    if m % size != 0:\n",
    "        x = trainx[:, n_mb :]\n",
    "        y = trainy[:, n_mb :]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    return minibatch_x, minibatch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_prop(params, x, Y):\n",
    "    A_prev = np.array(x, copy = True)\n",
    "    m = x.shape[1]\n",
    "    L = len(params)//2\n",
    "    caches = []\n",
    "    for l in range(L - 1):\n",
    "        \n",
    "        w = params['w' + str(l+1)]\n",
    "        b = params['b' + str(l+1)]\n",
    "        \n",
    "        \n",
    "        assert(w.shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(b.shape == (layer_dims[l + 1], 1))\n",
    "        assert(A_prev.shape == (layer_dims[l], m))\n",
    "            \n",
    "            \n",
    "        z = np.dot(w, A_prev) + b\n",
    "        cache = (A_prev, w, b, z)\n",
    "        A_prev = relu(z)\n",
    "        \n",
    "        \n",
    "        assert(z.shape == (layer_dims[l + 1], m))\n",
    "        \n",
    "        caches.append(cache)\n",
    "    \n",
    "        \n",
    "    w = params[\"w\" + str(L)]\n",
    "    b = params[\"b\" + str(L)]\n",
    "    \n",
    "    assert(w.shape == (layer_dims[L], layer_dims[L - 1]))\n",
    "    assert(b.shape == (layer_dims[L], 1))\n",
    "    assert(A_prev.shape == (layer_dims[L-1], m))\n",
    "    z = np.dot(w, A_prev) + b\n",
    "    \n",
    "    assert(z.shape == (layer_dims[L], m))\n",
    "    \n",
    "    cache = (A_prev, w, b, z)\n",
    "    AL = sigmoid(z)\n",
    "    \n",
    "    cost = -(np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)))/m\n",
    "    \n",
    "    caches.append(cache)\n",
    "    return cost, AL, caches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(AL, Y, caches):\n",
    "    m = Y.shape[1]\n",
    "    dA = np.divide(-Y, AL) + np.divide(1 - Y, 1 - AL)\n",
    "    assert(dA.shape == (layer_dims[-1], m))\n",
    "    grads = []\n",
    "    \n",
    "    L = len(caches)\n",
    "    A_prev, w, b, z = caches[L -1]\n",
    "    \n",
    "    dZ = sigmoid_back(dA, z)\n",
    "    dw = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(dZ.shape == z.shape)\n",
    "    dA_prev = np.dot(w.T, dZ)\n",
    "    dA = dA_prev\n",
    "    \n",
    "    grad = {\"dZ\" + str(L): dZ,\n",
    "           \"dw\" + str(L): dw,\n",
    "           \"db\" + str(L): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#     grad = (dZ, dw, db, dA_prev)\n",
    "    grads.append(grad)\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        A_prev, w, b, z = caches[l]\n",
    "        \n",
    "        dZ = relu_back(dA, z)\n",
    "        dw = np.dot(dZ, A_prev.T)/m\n",
    "        db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "        assert(db.shape == b.shape)\n",
    "        assert(dw.shape == w.shape)\n",
    "        assert(dZ.shape == z.shape)\n",
    "        dA_prev = np.dot(w.T, dZ)\n",
    "        dA = dA_prev\n",
    "        \n",
    "        grad = {\"dZ\" + str(l + 1): dZ,\n",
    "           \"dw\" + str(l + 1): dw,\n",
    "           \"db\" + str(l + 1): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#         grad = (dZ, dw, db, dA_prev)\n",
    "        grads.append(grad)\n",
    "        \n",
    "        \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(params, grads, X, Y, epsilon = 1e-7):\n",
    "    theta_values, keys_theta = gc.dictionary_to_vector(params)\n",
    "    grad = gc.gradient_to_vector(grads)\n",
    "    L = theta_values.shape[0]\n",
    "    grad_approx = np.zeros((L, 1))\n",
    "    \n",
    "    for i in range(L):\n",
    "        theta_plus = np.array(theta_values, copy = True)\n",
    "        theta_plus[i][0] = theta_plus[i][0] + epsilon\n",
    "        J_plus,_,_ = for_prop(gc.vector_to_dictionary(theta_plus, keys_theta, params), X, Y)\n",
    "        \n",
    "        theta_minus = np.array(theta_values, copy = True)\n",
    "        theta_minus[i][0] = theta_minus[i][0] - epsilon\n",
    "        J_minus,_,_ = for_prop(gc.vector_to_dictionary(theta_minus, keys_theta, params), X, Y)\n",
    "        \n",
    "        grad_approx[i] = (J_plus - J_minus)/(2 * epsilon)\n",
    "        \n",
    "    numerator = np.linalg.norm(grad - grad_approx)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        print(\"There is a mistake in back propagation, difference: \", str(difference))\n",
    "    else:\n",
    "        print(\"You nailed it, back propagation is all perfect Ready to GO..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(params, grads, v, s, beta1, beta2, t, epsilon, learning_rate):\n",
    "    L = len(params)//2\n",
    "    # if you use gradient checking then grads.reverse() is performed in the gradient_to_vector method in grad_check so don't need to call the reverse method in here\n",
    "    \n",
    "    #if you are not using gradient checking then you have to reverse grads here...\n",
    "    grads.reverse()\n",
    "    \n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        grad = grads[l]\n",
    "        \n",
    "        v[\"dw\" + str(l + 1)] = (beta1 * v[\"dw\" + str(l + 1)]) + ((1 - beta1) * grad[\"dw\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = (beta1 * v[\"db\" + str(l + 1)]) + ((1 - beta1) * grad[\"db\" + str(l + 1)])\n",
    "        \n",
    "        v_corrected[\"dw\" + str(l + 1)] = v[\"dw\" + str(l + 1)]/(1 - beta1**t)\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)]/(1 - beta1**t)\n",
    "        \n",
    "        s[\"dw\" + str(l + 1)] = (beta2 * s[\"dw\" + str(l + 1)]) + ((1 - beta2) * (grad[\"dw\" + str(l + 1)]**2))\n",
    "        s[\"db\" + str(l + 1)] = (beta2 * s[\"db\" + str(l + 1)]) + ((1 - beta2) * (grad[\"db\" + str(l + 1)]**2))\\\n",
    "        \n",
    "        s_corrected[\"dw\" + str(l + 1)] = s[\"dw\" + str(l + 1)]/(1 - beta2**t)\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)]/(1 - beta2**t)\n",
    "        \n",
    "        params[\"w\" + str(l + 1)] = params[\"w\" + str(l + 1)] - (learning_rate * (v_corrected[\"dw\" + str(l + 1)]/np.sqrt(s_corrected[\"dw\" + str(l + 1)] + epsilon)))\n",
    "        params[\"b\" + str(l + 1)] = params[\"b\" + str(l + 1)] - (learning_rate * (v_corrected[\"db\" + str(l + 1)]/np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)))\n",
    "        \n",
    "        \n",
    "        assert(params['w' + str(l + 1)].shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(params['b' + str(l + 1)].shape == (layer_dims[l + 1], 1))\n",
    "        \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, x, y):\n",
    "    cost, AL, caches = for_prop(params, x, y)\n",
    "    \n",
    "    y_prediction = np.where(AL<=0.5, 0, 1)\n",
    "    acc = 100 - (np.mean(np.abs(y_prediction - y)) * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(trianx, trainy, testx, testy, layer_dims, num_iteration, learning_rate, minibatch_size, print_cost, beta1 = 0.9, beta2 = 0.999, epsilon = 10e-8):\n",
    "    params = initialize(layer_dims)\n",
    "    v, s = initialize_adam(params)\n",
    "    \n",
    "    costs = []\n",
    "    for i in range(1, num_iteration+1):\n",
    "        \n",
    "        minibatch_x, minibatch_y = create_minibatch(trainx, trainy, minibatch_size)\n",
    "        cost_minibatch = 0\n",
    "        t = 0\n",
    "        for l in range(len(minibatch_x)):\n",
    "            cost, AL, caches = for_prop(params, minibatch_x[l], minibatch_y[l])\n",
    "            cost_minibatch += cost\n",
    "            grads = back_prop(AL, minibatch_y[l], caches)\n",
    "    #         gradient_check(params, grads, trainx, trainy, 1e-7)\n",
    "            t = t + 1\n",
    "            params = optimize(params, grads, v, s, beta1, beta2, t, epsilon, learning_rate)\n",
    "        \n",
    "        cost_minibatch = cost_minibatch / len(minibatch_x)\n",
    "\n",
    "            \n",
    "        if i%100==0 and print_cost:\n",
    "            print(\"cost at iteration \",i ,\": \",cost)\n",
    "            costs.append(cost_minibatch)\n",
    "            \n",
    "    train_acc = predict(params, trainx, trainy)\n",
    "    test_acc = predict(params, testx, testy)\n",
    "    \n",
    "    print(\"Training accuracy: \", train_acc)\n",
    "    print(\"Test accuracy: \", test_acc)\n",
    "    \n",
    "    data = {\"params\": params,\n",
    "           \"costs\": costs,\n",
    "            \"grads\": grads,\n",
    "           \"train_acc\": train_acc,\n",
    "           \"test_acc\": test_acc}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fulalpha.pickle\", \"rb\") as pickle_in:\n",
    "    data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['trainx', 'trainy_c', 'testx', 'testy_c'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data[\"trainx\"]\n",
    "trainy = data[\"trainy_c\"]\n",
    "testx = data[\"testx\"]\n",
    "testy = data[\"testy_c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 13000)\n",
      "(26, 13000)\n",
      "(784, 260)\n",
      "(26, 260)\n"
     ]
    }
   ],
   "source": [
    "print(trainx.shape)\n",
    "print(trainy.shape)\n",
    "print(testx.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 338000 into shape (4,13000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-a01fb3d53800>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrainx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermutation_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrainy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermutation_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtestx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermutation_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtesty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtesty\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermutation_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 338000 into shape (4,13000)"
     ]
    }
   ],
   "source": [
    "permutation_train = np.random.permutation(trainx.shape[1])\n",
    "permutation_test = np.random.permutation(testx.shape[1])\n",
    "\n",
    "trainx = trainx[:, permutation_train]\n",
    "trainy = trainy[:, permutation_train].reshape(4, trainx.shape[1])\n",
    "testx = testx[:, permutation_test]\n",
    "testy = testy[:, permutation_test].reshape(4, testx.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1954c71fcc8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 108\n",
    "print(chr(trainy[:, index] + ord('A')))\n",
    "img = trainx[:, index]\n",
    "img = img.reshape(28, 28)\n",
    "plt.imshow(img, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data['trainx']/255\n",
    "trainy = data['trainy']\n",
    "testx = data['testx']/255\n",
    "testy = data['testy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration  100 :  0.016501484608129342\n",
      "cost at iteration  200 :  0.0032855186304238475\n",
      "cost at iteration  300 :  0.001456610767869413\n",
      "cost at iteration  400 :  0.000846751609345783\n",
      "cost at iteration  500 :  0.0005730625213230605\n",
      "cost at iteration  600 :  0.0004249391685008731\n",
      "cost at iteration  700 :  0.00033403408138180975\n",
      "cost at iteration  800 :  0.0002732527515631019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in less_equal\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration  900 :  nan\n",
      "cost at iteration  1000 :  nan\n",
      "Training accuracy:  49.97501249375313\n",
      "Test accuracy:  50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in less_equal\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in less_equal\n"
     ]
    }
   ],
   "source": [
    "#use small neural network for a simple problems\n",
    "#if you use larger neural network then \n",
    "d = model(trainx, trainy, testx, testy, layer_dims, 1000, 0.05, 128, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN_result.pickle\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['train_acc'])\n",
    "print(d['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN ABClassifier result\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

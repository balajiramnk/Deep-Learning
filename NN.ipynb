{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from grad_check.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import import_ipynb\n",
    "import grad_check as gc\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [28 * 28, 4, 3, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    t = np.exp(z)\n",
    "    z = np.sum(z, axis = 0)\n",
    "    return np.divide(t, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_back(dA, z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[z<=0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layer_dims):\n",
    "    L = len(layer_dims)\n",
    "    params = {}\n",
    "    for l in range(1, L):\n",
    "        params[\"w\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * math.sqrt(2/layer_dims[l-1])\n",
    "        params[\"b\" + str(l)] = np.zeros([layer_dims[l], 1])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(params):\n",
    "    v = {}\n",
    "    s = {}\n",
    "    L = len(params)//2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dw\" + str(l)] = np.zeros(params[\"w\" + str(l)].shape)\n",
    "        v[\"db\" + str(l)] = np.zeros(params[\"b\" + str(l)].shape)\n",
    "        \n",
    "        s[\"dw\" + str(l)] = np.zeros(params[\"w\" + str(l)].shape)\n",
    "        s[\"db\" + str(l)] = np.zeros(params[\"b\" + str(l)].shape)\n",
    "        \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(trainx, trainy, size):\n",
    "    m = trainx.shape[1]\n",
    "    n_mb = math.floor(m / size)\n",
    "    minibatch_x = []\n",
    "    minibatch_y = []\n",
    "    \n",
    "    for i in range(n_mb):\n",
    "        x = trainx[:, i * size : (i + 1) * size]\n",
    "        y = trainy[:, i * size : (i + 1) * size]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    if m % size != 0:\n",
    "        x = trainx[:, n_mb :]\n",
    "        y = trainy[:, n_mb :]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    return minibatch_x, minibatch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_prop(params, x, Y):\n",
    "    A_prev = np.array(x, copy = True)\n",
    "    m = x.shape[1]\n",
    "    L = len(params)//2\n",
    "    caches = []\n",
    "    for l in range(L - 1):\n",
    "        \n",
    "        w = params['w' + str(l+1)]\n",
    "        b = params['b' + str(l+1)]\n",
    "        \n",
    "        \n",
    "        assert(w.shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(b.shape == (layer_dims[l + 1], 1))\n",
    "        assert(A_prev.shape == (layer_dims[l], m))\n",
    "            \n",
    "            \n",
    "        z = np.dot(w, A_prev) + b\n",
    "        cache = (A_prev, w, b, z)\n",
    "        A_prev = relu(z)\n",
    "        \n",
    "        \n",
    "        assert(z.shape == (layer_dims[l + 1], m))\n",
    "        \n",
    "        caches.append(cache)\n",
    "    \n",
    "        \n",
    "    w = params[\"w\" + str(L)]\n",
    "    b = params[\"b\" + str(L)]\n",
    "    \n",
    "    assert(w.shape == (layer_dims[L], layer_dims[L - 1]))\n",
    "    assert(b.shape == (layer_dims[L], 1))\n",
    "    assert(A_prev.shape == (layer_dims[L-1], m))\n",
    "    z = np.dot(w, A_prev) + b\n",
    "    \n",
    "    assert(z.shape == (layer_dims[L], m))\n",
    "    \n",
    "    cache = (A_prev, w, b, z)\n",
    "    AL = softmax(z)\n",
    "    \n",
    "    cost = -(np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)))/m\n",
    "    \n",
    "    caches.append(cache)\n",
    "    return cost, AL, caches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(AL, Y, caches):\n",
    "    m = Y.shape[1]\n",
    "    grads = []\n",
    "    \n",
    "    L = len(caches)\n",
    "    A_prev, w, b, z = caches[L -1]\n",
    "    \n",
    "    dZ = AL - Y\n",
    "    dw = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(dZ.shape == z.shape)\n",
    "    dA_prev = np.dot(w.T, dZ)\n",
    "    dA = dA_prev\n",
    "    \n",
    "    grad = {\"dZ\" + str(L): dZ,\n",
    "           \"dw\" + str(L): dw,\n",
    "           \"db\" + str(L): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#     grad = (dZ, dw, db, dA_prev)\n",
    "    grads.append(grad)\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        A_prev, w, b, z = caches[l]\n",
    "        \n",
    "        dZ = relu_back(dA, z)\n",
    "        dw = np.dot(dZ, A_prev.T)/m\n",
    "        db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "        assert(db.shape == b.shape)\n",
    "        assert(dw.shape == w.shape)\n",
    "        assert(dZ.shape == z.shape)\n",
    "        dA_prev = np.dot(w.T, dZ)\n",
    "        dA = dA_prev\n",
    "        \n",
    "        grad = {\"dZ\" + str(l + 1): dZ,\n",
    "           \"dw\" + str(l + 1): dw,\n",
    "           \"db\" + str(l + 1): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#         grad = (dZ, dw, db, dA_prev)\n",
    "        grads.append(grad)\n",
    "        \n",
    "        \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(params, grads, X, Y, epsilon = 1e-7):\n",
    "    theta_values, keys_theta = gc.dictionary_to_vector(params)\n",
    "    grad = gc.gradient_to_vector(grads)\n",
    "    L = theta_values.shape[0]\n",
    "    grad_approx = np.zeros((L, 1))\n",
    "    \n",
    "    for i in range(L):\n",
    "        theta_plus = np.array(theta_values, copy = True)\n",
    "        theta_plus[i][0] = theta_plus[i][0] + epsilon\n",
    "        J_plus,_,_ = for_prop(gc.vector_to_dictionary(theta_plus, keys_theta, params), X, Y)\n",
    "        \n",
    "        theta_minus = np.array(theta_values, copy = True)\n",
    "        theta_minus[i][0] = theta_minus[i][0] - epsilon\n",
    "        J_minus,_,_ = for_prop(gc.vector_to_dictionary(theta_minus, keys_theta, params), X, Y)\n",
    "        \n",
    "        grad_approx[i] = (J_plus - J_minus)/(2 * epsilon)\n",
    "        \n",
    "    numerator = np.linalg.norm(grad - grad_approx)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        print(\"There is a mistake in back propagation, difference: \", str(difference))\n",
    "    else:\n",
    "        print(\"You nailed it, back propagation is all perfect Ready to GO..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(params, grads, v, s, beta1, beta2, t, epsilon, learning_rate):\n",
    "    L = len(params)//2\n",
    "    # if you use gradient checking then grads.reverse() is performed in the gradient_to_vector method in grad_check so don't need to call the reverse method in here\n",
    "    \n",
    "    #if you are not using gradient checking then you have to reverse grads here...\n",
    "    grads.reverse()\n",
    "    \n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        grad = grads[l]\n",
    "        \n",
    "        v[\"dw\" + str(l + 1)] = (beta1 * v[\"dw\" + str(l + 1)]) + ((1 - beta1) * grad[\"dw\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = (beta1 * v[\"db\" + str(l + 1)]) + ((1 - beta1) * grad[\"db\" + str(l + 1)])\n",
    "        \n",
    "        v_corrected[\"dw\" + str(l + 1)] = v[\"dw\" + str(l + 1)]/(1 - beta1**t)\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)]/(1 - beta1**t)\n",
    "        \n",
    "        s[\"dw\" + str(l + 1)] = (beta2 * s[\"dw\" + str(l + 1)]) + ((1 - beta2) * (grad[\"dw\" + str(l + 1)]**2))\n",
    "        s[\"db\" + str(l + 1)] = (beta2 * s[\"db\" + str(l + 1)]) + ((1 - beta2) * (grad[\"db\" + str(l + 1)]**2))\\\n",
    "        \n",
    "        s_corrected[\"dw\" + str(l + 1)] = s[\"dw\" + str(l + 1)]/(1 - beta2**t)\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)]/(1 - beta2**t)\n",
    "        \n",
    "        params[\"w\" + str(l + 1)] = params[\"w\" + str(l + 1)] - (learning_rate * (v_corrected[\"dw\" + str(l + 1)]/np.sqrt(s_corrected[\"dw\" + str(l + 1)] + epsilon)))\n",
    "        params[\"b\" + str(l + 1)] = params[\"b\" + str(l + 1)] - (learning_rate * (v_corrected[\"db\" + str(l + 1)]/np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)))\n",
    "        \n",
    "        \n",
    "        assert(params['w' + str(l + 1)].shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(params['b' + str(l + 1)].shape == (layer_dims[l + 1], 1))\n",
    "        \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, x, y):\n",
    "    cost, AL, caches = for_prop(params, x, y)\n",
    "    \n",
    "    Y_prediction = np.zeros(AL.shape)\n",
    "    for i in range(AL.shape[1]):\n",
    "        Y_prediction[np.argmax(AL[:, i]), i] = 1\n",
    "    \n",
    "    acc = ((Y_prediction * y).sum()/y.shape[1]) * 100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(trianx, trainy, testx, testy, layer_dims, num_iteration, learning_rate, minibatch_size, print_cost, beta1 = 0.9, beta2 = 0.999, epsilon = 10e-8):\n",
    "    params = initialize(layer_dims)\n",
    "    v, s = initialize_adam(params)\n",
    "    \n",
    "    costs = []\n",
    "    for i in range(1, num_iteration+1):\n",
    "        \n",
    "        minibatch_x, minibatch_y = create_minibatch(trainx, trainy, minibatch_size)\n",
    "        cost_minibatch = 0\n",
    "        t = 0\n",
    "        for l in range(len(minibatch_x)):\n",
    "            cost, AL, caches = for_prop(params, minibatch_x[l], minibatch_y[l])\n",
    "            cost_minibatch += cost\n",
    "            grads = back_prop(AL, minibatch_y[l], caches)\n",
    "    #         gradient_check(params, grads, trainx, trainy, 1e-7)\n",
    "            t = t + 1\n",
    "            params = optimize(params, grads, v, s, beta1, beta2, t, epsilon, learning_rate)\n",
    "        \n",
    "        cost_minibatch = cost_minibatch / len(minibatch_x)\n",
    "\n",
    "            \n",
    "        if i%100==0 and print_cost:\n",
    "            print(\"cost at iteration \",i ,\": \",cost)\n",
    "            costs.append(cost_minibatch)\n",
    "            \n",
    "    train_acc = predict(params, trainx, trainy)\n",
    "    test_acc = predict(params, testx, testy)\n",
    "    \n",
    "    print(\"Training accuracy: \", train_acc)\n",
    "    print(\"Test accuracy: \", test_acc)\n",
    "    \n",
    "    data = {\"params\": params,\n",
    "           \"costs\": costs,\n",
    "            \"grads\": grads,\n",
    "           \"train_acc\": train_acc,\n",
    "           \"test_acc\": test_acc}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fulalpha.pickle\", \"rb\") as pickle_in:\n",
    "    data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['trainx', 'trainy_c', 'testx', 'testy_c'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data[\"trainx\"]\n",
    "trainy = data[\"trainy_c\"]\n",
    "testx = data[\"testx\"]\n",
    "testy = data[\"testy_c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 13000)\n",
      "(26, 13000)\n",
      "(784, 260)\n",
      "(26, 260)\n"
     ]
    }
   ],
   "source": [
    "print(trainx.shape)\n",
    "print(trainy.shape)\n",
    "print(testx.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_train = np.random.permutation(trainx.shape[1])\n",
    "permutation_test = np.random.permutation(testx.shape[1])\n",
    "\n",
    "trainx = trainx[:, permutation_train]\n",
    "trainy = trainy[:, permutation_train]\n",
    "testx = testx[:, permutation_test]\n",
    "testy = testy[:, permutation_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x226f2c27688>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 108\n",
    "img = trainx[:, index]\n",
    "img = img.reshape(28, 28)\n",
    "plt.imshow(img, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data['trainx']/255\n",
    "trainy = data['trainy_c']\n",
    "testx = data['testx']/255\n",
    "testy = data['testy_c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in log\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration  100 :  nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-b324520d7fd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#use small neural network for a simple problems\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#if you use larger neural network then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-6aee34fcb9b3>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(trianx, trainy, testx, testy, layer_dims, num_iteration, learning_rate, minibatch_size, print_cost, beta1, beta2, epsilon)\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfor_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mcost_minibatch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m#         gradient_check(params, grads, trainx, trainy, 1e-7)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-8354a7ba83e6>\u001b[0m in \u001b[0;36mback_prop\u001b[1;34m(AL, Y, caches)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mdA_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mdA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdA_prev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#use small neural network for a simple problems\n",
    "#if you use larger neural network then \n",
    "d = model(trainx, trainy, testx, testy, layer_dims, 1000, 0.05, 1000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN_result_alpha.pickle\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['train_acc'])\n",
    "print(d['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN ABClassifier result\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from grad_check.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import import_ipynb\n",
    "import grad_check as gc\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [28 * 28, 50, 50, 26, 26]\n",
    "keep_prob = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_back(dA, z):\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    return dA * s*(1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    t = np.exp(z)\n",
    "    z = np.sum(t, axis = 0)\n",
    "    return np.divide(t, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_back(dA, z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[z<=0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layer_dims):\n",
    "    L = len(layer_dims)\n",
    "    params = {}\n",
    "    for l in range(1, L):\n",
    "        params[\"w\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * math.sqrt(2/layer_dims[l-1])\n",
    "        params[\"b\" + str(l)] = np.zeros([layer_dims[l], 1])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(params):\n",
    "    v = {}\n",
    "    s = {}\n",
    "    L = len(params)//2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dw\" + str(l)] = np.zeros(params[\"w\" + str(l)].shape)\n",
    "        v[\"db\" + str(l)] = np.zeros(params[\"b\" + str(l)].shape)\n",
    "        \n",
    "        s[\"dw\" + str(l)] = np.zeros(params[\"w\" + str(l)].shape)\n",
    "        s[\"db\" + str(l)] = np.zeros(params[\"b\" + str(l)].shape)\n",
    "        \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(trainx, trainy, size):\n",
    "    m = trainx.shape[1]\n",
    "    n_mb = math.floor(m / size)\n",
    "    minibatch_x = []\n",
    "    minibatch_y = []\n",
    "    \n",
    "    for i in range(n_mb):\n",
    "        x = trainx[:, i * size : (i + 1) * size]\n",
    "        y = trainy[:, i * size : (i + 1) * size]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    if m % size != 0:\n",
    "        x = trainx[:, n_mb :]\n",
    "        y = trainy[:, n_mb :]\n",
    "        \n",
    "        minibatch_x.append(x)\n",
    "        minibatch_y.append(y)\n",
    "        \n",
    "    return minibatch_x, minibatch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_prop(params, x, Y, lambd = 0.7):\n",
    "    A_prev = np.array(x, copy = True)\n",
    "    m = x.shape[1]\n",
    "    L = len(params)//2\n",
    "    caches = []\n",
    "    for l in range(L - 1):\n",
    "        \n",
    "        w = params['w' + str(l+1)]\n",
    "        b = params['b' + str(l+1)]\n",
    "        \n",
    "        \n",
    "        assert(w.shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(b.shape == (layer_dims[l + 1], 1))\n",
    "        assert(A_prev.shape == (layer_dims[l], m))\n",
    "            \n",
    "            \n",
    "        z = np.dot(w, A_prev) + b\n",
    "        cache = (A_prev, w, b, z)\n",
    "        A_prev = relu(z)\n",
    "        \n",
    "        \n",
    "        assert(z.shape == (layer_dims[l + 1], m))\n",
    "        \n",
    "        caches.append(cache)\n",
    "    \n",
    "        \n",
    "    w = params[\"w\" + str(L)]\n",
    "    b = params[\"b\" + str(L)]\n",
    "    \n",
    "    assert(w.shape == (layer_dims[L], layer_dims[L - 1]))\n",
    "    assert(b.shape == (layer_dims[L], 1))\n",
    "    assert(A_prev.shape == (layer_dims[L-1], m))\n",
    "    z = np.dot(w, A_prev) + b\n",
    "    \n",
    "    assert(z.shape == (layer_dims[L], m))\n",
    "    \n",
    "    cache = (A_prev, w, b, z)\n",
    "    AL = softmax(z)\n",
    "    \n",
    "    WL = 0\n",
    "    for key in params.keys():\n",
    "        WL = WL + np.sum(np.square(params[key]))\n",
    "    \n",
    "    \n",
    "    cost = -(np.sum(Y * np.log(AL)))/m + ((1/m) * (lambd/2) * WL)\n",
    "    \n",
    "    caches.append(cache)\n",
    "    return cost, AL, caches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(AL, Y, caches, lambd = 0.7):\n",
    "    m = Y.shape[1]\n",
    "    grads = []\n",
    "    \n",
    "    L = len(caches)\n",
    "    A_prev, w, b, z = caches[L -1]\n",
    "    \n",
    "    dZ = AL - Y\n",
    "    dw = np.dot(dZ, A_prev.T)/m + ((lambd/m) * w)\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(dZ.shape == z.shape)\n",
    "    dA_prev = np.dot(w.T, dZ)\n",
    "    dA = dA_prev\n",
    "    \n",
    "    grad = {\"dZ\" + str(L): dZ,\n",
    "           \"dw\" + str(L): dw,\n",
    "           \"db\" + str(L): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#     grad = (dZ, dw, db, dA_prev)\n",
    "    grads.append(grad)\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        A_prev, w, b, z = caches[l]\n",
    "        \n",
    "        dZ = relu_back(dA, z)\n",
    "        dw = np.dot(dZ, A_prev.T)/m + ((lambd/m) * w)\n",
    "        db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "        assert(db.shape == b.shape)\n",
    "        assert(dw.shape == w.shape)\n",
    "        assert(dZ.shape == z.shape)\n",
    "        dA_prev = np.dot(w.T, dZ)\n",
    "        dA = dA_prev\n",
    "        \n",
    "        grad = {\"dZ\" + str(l + 1): dZ,\n",
    "           \"dw\" + str(l + 1): dw,\n",
    "           \"db\" + str(l + 1): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#         grad = (dZ, dw, db, dA_prev)\n",
    "        grads.append(grad)\n",
    "        \n",
    "        \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(params, grads, X, Y, epsilon = 1e-7):\n",
    "    theta_values, keys_theta = gc.dictionary_to_vector(params)\n",
    "    grad = gc.gradient_to_vector(grads)\n",
    "    L = theta_values.shape[0]\n",
    "    grad_approx = np.zeros((L, 1))\n",
    "    \n",
    "    for i in range(L):\n",
    "        theta_plus = np.array(theta_values, copy = True)\n",
    "        theta_plus[i][0] = theta_plus[i][0] + epsilon\n",
    "        J_plus,_,_ = for_prop(gc.vector_to_dictionary(theta_plus, keys_theta, params), X, Y)\n",
    "        \n",
    "        theta_minus = np.array(theta_values, copy = True)\n",
    "        theta_minus[i][0] = theta_minus[i][0] - epsilon\n",
    "        J_minus,_,_ = for_prop(gc.vector_to_dictionary(theta_minus, keys_theta, params), X, Y)\n",
    "        \n",
    "        grad_approx[i] = (J_plus - J_minus)/(2 * epsilon)\n",
    "        \n",
    "    numerator = np.linalg.norm(grad - grad_approx)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        print(\"There is a mistake in back propagation, difference: \", str(difference))\n",
    "    else:\n",
    "        print(\"You nailed it, back propagation is all perfect Ready to GO..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(params, grads, v, s, beta1, beta2, t, epsilon, learning_rate):\n",
    "    L = len(params)//2\n",
    "    # if you use gradient checking then grads.reverse() is performed in the gradient_to_vector method in grad_check so don't need to call the reverse method in here\n",
    "    \n",
    "    #if you are not using gradient checking then you have to reverse grads here...\n",
    "    grads.reverse()\n",
    "    \n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        grad = grads[l]\n",
    "        \n",
    "        v[\"dw\" + str(l + 1)] = (beta1 * v[\"dw\" + str(l + 1)]) + ((1 - beta1) * grad[\"dw\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = (beta1 * v[\"db\" + str(l + 1)]) + ((1 - beta1) * grad[\"db\" + str(l + 1)])\n",
    "        \n",
    "        v_corrected[\"dw\" + str(l + 1)] = v[\"dw\" + str(l + 1)]/(1 - beta1**t)\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)]/(1 - beta1**t)\n",
    "        \n",
    "        s[\"dw\" + str(l + 1)] = (beta2 * s[\"dw\" + str(l + 1)]) + ((1 - beta2) * (grad[\"dw\" + str(l + 1)]**2))\n",
    "        s[\"db\" + str(l + 1)] = (beta2 * s[\"db\" + str(l + 1)]) + ((1 - beta2) * (grad[\"db\" + str(l + 1)]**2))\\\n",
    "        \n",
    "        s_corrected[\"dw\" + str(l + 1)] = s[\"dw\" + str(l + 1)]/(1 - beta2**t)\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)]/(1 - beta2**t)\n",
    "        \n",
    "        params[\"w\" + str(l + 1)] = params[\"w\" + str(l + 1)] - (learning_rate * (v_corrected[\"dw\" + str(l + 1)]/np.sqrt(s_corrected[\"dw\" + str(l + 1)] + epsilon)))\n",
    "        params[\"b\" + str(l + 1)] = params[\"b\" + str(l + 1)] - (learning_rate * (v_corrected[\"db\" + str(l + 1)]/np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)))\n",
    "        \n",
    "        \n",
    "        assert(params['w' + str(l + 1)].shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(params['b' + str(l + 1)].shape == (layer_dims[l + 1], 1))\n",
    "        \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, x, y):\n",
    "    cost, AL, caches = for_prop(params, x, y)\n",
    "    \n",
    "    Y_prediction = np.zeros(AL.shape)\n",
    "    for i in range(AL.shape[1]):\n",
    "        Y_prediction[np.argmax(AL[:, i]), i] = 1\n",
    "    \n",
    "    acc = ((Y_prediction * y).sum()/y.shape[1]) * 100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(trianx, trainy, testx, testy, layer_dims, num_iteration, learning_rate, minibatch_size, print_cost, beta1 = 0.9, beta2 = 0.999, epsilon = 10e-8):\n",
    "    params = initialize(layer_dims)\n",
    "    v, s = initialize_adam(params)\n",
    "    \n",
    "    costs = []\n",
    "    for i in range(1, num_iteration+1):\n",
    "        \n",
    "        minibatch_x, minibatch_y = create_minibatch(trainx, trainy, minibatch_size)\n",
    "        cost_minibatch = 0\n",
    "        t = 0\n",
    "        for l in range(len(minibatch_x)):\n",
    "            cost, AL, caches = for_prop(params, minibatch_x[l], minibatch_y[l])\n",
    "            cost_minibatch += cost\n",
    "            grads = back_prop(AL, minibatch_y[l], caches)\n",
    "    #         gradient_check(params, grads, trainx, trainy, 1e-7)\n",
    "            t = t + 1\n",
    "            params = optimize(params, grads, v, s, beta1, beta2, t, epsilon, learning_rate)\n",
    "        \n",
    "        cost_minibatch = cost_minibatch / len(minibatch_x)\n",
    "\n",
    "            \n",
    "        if i%100==0 and print_cost:\n",
    "            print(\"cost at iteration \",i ,\": \",cost)\n",
    "            costs.append(cost_minibatch)\n",
    "            \n",
    "    train_acc = predict(params, trainx, trainy)\n",
    "    test_acc = predict(params, testx, testy)\n",
    "    \n",
    "    print(\"Training accuracy: \", train_acc)\n",
    "    print(\"Test accuracy: \", test_acc)\n",
    "    \n",
    "    data = {\"params\": params,\n",
    "           \"costs\": costs,\n",
    "            \"grads\": grads,\n",
    "           \"train_acc\": train_acc,\n",
    "           \"test_acc\": test_acc}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fulalpha_500.pickle\", \"rb\") as pickle_in:\n",
    "    data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['trainx', 'trainy', 'testx', 'testy'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data[\"trainx\"]\n",
    "trainy = data[\"trainy\"]\n",
    "testx = data[\"testx\"]\n",
    "testy = data[\"testy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 13000)\n",
      "(26, 26000)\n",
      "(784, 520)\n",
      "(26, 5083)\n"
     ]
    }
   ],
   "source": [
    "print(trainx.shape)\n",
    "print(trainy.shape)\n",
    "print(testx.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17de0e59288>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 108\n",
    "img = trainx[:, index]\n",
    "img = img.reshape(28, 28)\n",
    "plt.imshow(img, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data['trainx']/255\n",
    "trainy = data['trainy']\n",
    "testx = data['testx']/255\n",
    "testy = data['testy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_train = np.random.permutation(trainx.shape[1])\n",
    "permutation_test = np.random.permutation(testx.shape[1])\n",
    "\n",
    "trainx = trainx[:, permutation_train]\n",
    "trainy = trainy[:, permutation_train]\n",
    "testx = testx[:, permutation_test]\n",
    "testy = testy[:, permutation_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration  100 :  0.26656112507281715\n",
      "cost at iteration  200 :  0.20229333937516655\n",
      "cost at iteration  300 :  0.24841090626081935\n",
      "cost at iteration  400 :  0.1672836694941762\n",
      "cost at iteration  500 :  0.17577351268020103\n",
      "cost at iteration  600 :  0.17001238662254523\n",
      "cost at iteration  700 :  0.17141367134447769\n",
      "cost at iteration  800 :  0.155194438434375\n",
      "Training accuracy:  94.58461538461539\n",
      "Test accuracy:  82.5\n"
     ]
    }
   ],
   "source": [
    "#use small neural network for a simple problems\n",
    "#if you use larger neural network then \n",
    "d = model(trainx, trainy, testx, testy, layer_dims, 800, 0.05, 1024, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN_result_alpha.pickle\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.68076923076923\n",
      "55.12492622467047\n"
     ]
    }
   ],
   "source": [
    "print(d['train_acc'])\n",
    "print(d['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN ABClassifier result\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

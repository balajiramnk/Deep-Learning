{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import import_ipynb\n",
    "import grad_check as gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [28 * 28, 4, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_back(dA, z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return dA * s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_back(dA, z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[z<=0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layer_dims):\n",
    "    L = len(layer_dims)\n",
    "    params = {}\n",
    "    for l in range(1, L):\n",
    "        params[\"w\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * math.sqrt(2/layer_dims[l-1])\n",
    "        params[\"b\" + str(l)] = np.zeros([layer_dims[l], 1])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_prop(params, x, Y):\n",
    "    A_prev = np.array(x, copy = True)\n",
    "    m = x.shape[1]\n",
    "    L = len(params)//2\n",
    "    caches = []\n",
    "    for l in range(L - 1):\n",
    "        \n",
    "        w = params['w' + str(l+1)]\n",
    "        b = params['b' + str(l+1)]\n",
    "        \n",
    "        \n",
    "        assert(w.shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(b.shape == (layer_dims[l + 1], 1))\n",
    "        assert(A_prev.shape == (layer_dims[l], m))\n",
    "            \n",
    "            \n",
    "        z = np.dot(w, A_prev) + b\n",
    "        cache = (A_prev, w, b, z)\n",
    "        A_prev = relu(z)\n",
    "        \n",
    "        \n",
    "        assert(z.shape == (layer_dims[l + 1], m))\n",
    "        \n",
    "        caches.append(cache)\n",
    "    \n",
    "        \n",
    "    w = params[\"w\" + str(L)]\n",
    "    b = params[\"b\" + str(L)]\n",
    "    \n",
    "    assert(w.shape == (layer_dims[L], layer_dims[L - 1]))\n",
    "    assert(b.shape == (layer_dims[L], 1))\n",
    "    assert(A_prev.shape == (layer_dims[L-1], m))\n",
    "    z = np.dot(w, A_prev) + b\n",
    "    \n",
    "    assert(z.shape == (layer_dims[L], m))\n",
    "    \n",
    "    cache = (A_prev, w, b, z)\n",
    "    AL = sigmoid(z)\n",
    "    \n",
    "    cost = -(np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)))/m\n",
    "    \n",
    "    caches.append(cache)\n",
    "    return cost, AL, caches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(AL, Y, caches):\n",
    "    m = Y.shape[1]\n",
    "    dA = np.divide(-Y, AL) + np.divide(1 - Y, 1 - AL)\n",
    "    assert(dA.shape == (layer_dims[-1], m))\n",
    "    grads = []\n",
    "    \n",
    "    L = len(caches)\n",
    "    A_prev, w, b, z = caches[L -1]\n",
    "    \n",
    "    dZ = sigmoid_back(dA, z)\n",
    "    dw = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(dZ.shape == z.shape)\n",
    "    dA_prev = np.dot(w.T, dZ)\n",
    "    dA = dA_prev\n",
    "    \n",
    "    grad = {\"dZ\" + str(L): dZ,\n",
    "           \"dw\" + str(L): dw,\n",
    "           \"db\" + str(L): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#     grad = (dZ, dw, db, dA_prev)\n",
    "    grads.append(grad)\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        A_prev, w, b, z = caches[l]\n",
    "        \n",
    "        dZ = relu_back(dA, z)\n",
    "        dw = np.dot(dZ, A_prev.T)/m\n",
    "        db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "        assert(db.shape == b.shape)\n",
    "        assert(dw.shape == w.shape)\n",
    "        assert(dZ.shape == z.shape)\n",
    "        dA_prev = np.dot(w.T, dZ)\n",
    "        dA = dA_prev\n",
    "        \n",
    "        grad = {\"dZ\" + str(l + 1): dZ,\n",
    "           \"dw\" + str(l + 1): dw,\n",
    "           \"db\" + str(l + 1): db,\n",
    "           \"dA_prev\" + str(L): dA_prev}\n",
    "#         grad = (dZ, dw, db, dA_prev)\n",
    "        grads.append(grad)\n",
    "        \n",
    "        \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(params, grads, X, Y, epsilon = 1e-7):\n",
    "    theta_values, keys_theta = gc.dictionary_to_vector(params)\n",
    "    grad = gc.gradient_to_vector(grads)\n",
    "    L = theta_values.shape[0]\n",
    "    grad_approx = np.zeros((L, 1))\n",
    "    \n",
    "    for i in range(L):\n",
    "        theta_plus = np.array(theta_values, copy = True)\n",
    "        theta_plus[i][0] = theta_plus[i][0] + epsilon\n",
    "        J_plus,_,_ = for_prop(gc.vector_to_dictionary(theta_plus, keys_theta, params), X, Y)\n",
    "        \n",
    "        theta_minus = np.array(theta_values, copy = True)\n",
    "        theta_minus[i][0] = theta_minus[i][0] - epsilon\n",
    "        J_minus,_,_ = for_prop(gc.vector_to_dictionary(theta_minus, keys_theta, params), X, Y)\n",
    "        \n",
    "        grad_approx[i] = (J_plus - J_minus)/(2 * epsilon)\n",
    "        \n",
    "    numerator = np.linalg.norm(grad - grad_approx)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        print(\"There is a mistake in back propagation, difference: \", str(difference))\n",
    "    else:\n",
    "        print(\"You nailed it, back propagation is all perfect Ready to GO..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(params, grads, learning_rate):\n",
    "    L = len(params)//2\n",
    "    # if you use gradient checking then grads.reverse() is performed in the gradient_to_vector method in grad_check so don't need to call the reverse method in here\n",
    "    \n",
    "    #if you are not using gradient checking then you have to reverse grads here...\n",
    "    grads.reverse()\n",
    "    \n",
    "    for l in range(L):\n",
    "        grad = grads[l]\n",
    "        dw = grad['dw' + str(l + 1)]\n",
    "        db = grad['db' + str(l + 1)]\n",
    "#         dZ, dw, db, dA_prev = grad\n",
    "        params['w' + str(l + 1)] = params['w' + str(l + 1)] - (learning_rate * dw)\n",
    "        params['b' + str(l + 1)] = params['b' + str(l + 1)] - (learning_rate * db)\n",
    "\n",
    "        assert(params['w' + str(l + 1)].shape == (layer_dims[l + 1], layer_dims[l]))\n",
    "        assert(params['b' + str(l + 1)].shape == (layer_dims[l + 1], 1))\n",
    "        \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, x, y):\n",
    "    cost, AL, caches = for_prop(params, x, y)\n",
    "    \n",
    "    y_prediction = np.where(AL<=0.5, 0, 1)\n",
    "    acc = 100 - (np.mean(np.abs(y_prediction - y)) * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(trianx, trainy, testx, testy, layer_dims, num_iteration, learning_rate, print_cost):\n",
    "    params = initialize(layer_dims)\n",
    "    \n",
    "    costs = []\n",
    "    for i in range(1, num_iteration+1):\n",
    "        \n",
    "        cost, AL, caches = for_prop(params, trainx, trainy)\n",
    "        grads = back_prop(AL, trainy, caches)\n",
    "#         gradient_check(params, grads, trainx, trainy, 1e-7)\n",
    "        params = optimize(params, grads, learning_rate)\n",
    "        \n",
    "        if i%100 == 0 and print_cost:\n",
    "            print(\"cost at iteration \", i, \": \", cost)\n",
    "            costs.append(cost)\n",
    "            \n",
    "    train_acc = predict(params, trainx, trainy)\n",
    "    test_acc = predict(params, testx, testy)\n",
    "    \n",
    "    print(\"Training accuracy: \", train_acc)\n",
    "    print(\"Test accuracy: \", test_acc)\n",
    "    \n",
    "    data = {\"params\": params,\n",
    "           \"costs\": costs,\n",
    "            \"grads\": grads,\n",
    "           \"train_acc\": train_acc,\n",
    "           \"test_acc\": test_acc}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"handwrittendata.pickle\", \"rb\") as pickle_in:\n",
    "    data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['trainx', 'trainy', 'testx', 'testy'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_train = np.random.permutation(trainx.shape[1])\n",
    "permutation_test = np.random.permutation(testx.shape[1])\n",
    "\n",
    "trainx = trainx[:, permutation_train]\n",
    "trainy = trainy[:, permutation_train].reshape(1, trainx.shape[1])\n",
    "testx = testx[:, permutation_test]\n",
    "testy = testy[:, permutation_test].reshape(1, testx.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29569e73888>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = trainx[:, 108]\n",
    "img = img.reshape(28, 28)\n",
    "plt.imshow(img, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = data['trainx']/255\n",
    "trainy = data['trainy']\n",
    "testx = data['testx']/255\n",
    "testy = data['testy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration  100 :  0.10343751698849722\n",
      "cost at iteration  200 :  0.05273764824512255\n",
      "cost at iteration  300 :  0.036610819568317285\n",
      "cost at iteration  400 :  0.02790595092196209\n",
      "cost at iteration  500 :  0.02208853089544132\n",
      "cost at iteration  600 :  0.01814963808648613\n",
      "cost at iteration  700 :  0.01527041425032196\n",
      "cost at iteration  800 :  0.013012203885317646\n",
      "cost at iteration  900 :  0.011166223290486117\n",
      "cost at iteration  1000 :  0.009680028778615148\n",
      "cost at iteration  1100 :  0.008472715430721107\n",
      "cost at iteration  1200 :  0.007478123079721291\n",
      "cost at iteration  1300 :  0.006608343291679104\n",
      "cost at iteration  1400 :  0.005866442114962686\n",
      "cost at iteration  1500 :  0.00525429712554995\n",
      "cost at iteration  1600 :  0.00473916796071052\n",
      "cost at iteration  1700 :  0.004298501181695989\n",
      "cost at iteration  1800 :  0.003922425725436762\n",
      "cost at iteration  1900 :  0.003597244907297193\n",
      "cost at iteration  2000 :  0.0033140887418698505\n",
      "Training accuracy:  100.0\n",
      "Test accuracy:  98.75\n"
     ]
    }
   ],
   "source": [
    "d = model(trainx, trainy, testx, testy, layer_dims, 2000, 0.05, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN_result.pickle\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['train_acc'])\n",
    "print(d['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NN ABClassifier result\", \"wb\") as pickle_out:\n",
    "    pickle.dump(d, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
